"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[3608],{3905:function(e,t,r){r.d(t,{Zo:function(){return u},kt:function(){return p}});var n=r(7294);function o(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function a(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,n)}return r}function i(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?a(Object(r),!0).forEach((function(t){o(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):a(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function s(e,t){if(null==e)return{};var r,n,o=function(e,t){if(null==e)return{};var r,n,o={},a=Object.keys(e);for(n=0;n<a.length;n++)r=a[n],t.indexOf(r)>=0||(o[r]=e[r]);return o}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(n=0;n<a.length;n++)r=a[n],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(o[r]=e[r])}return o}var l=n.createContext({}),c=function(e){var t=n.useContext(l),r=t;return e&&(r="function"==typeof e?e(t):i(i({},t),e)),r},u=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var r=e.components,o=e.mdxType,a=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),m=c(r),p=o,f=m["".concat(l,".").concat(p)]||m[p]||d[p]||a;return r?n.createElement(f,i(i({ref:t},u),{},{components:r})):n.createElement(f,i({ref:t},u))}));function p(e,t){var r=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var a=r.length,i=new Array(a);i[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:o,i[1]=s;for(var c=2;c<a;c++)i[c]=r[c];return n.createElement.apply(null,i)}return n.createElement.apply(null,r)}m.displayName="MDXCreateElement"},7115:function(e,t,r){r.r(t),r.d(t,{default:function(){return d},frontMatter:function(){return s},metadata:function(){return l},toc:function(){return c}});var n=r(7462),o=r(3366),a=(r(7294),r(3905)),i=["components"],s={id:"memory",title:"Memory Use"},l={unversionedId:"guides/advanced/memory",id:"guides/advanced/memory",isDocsHomePage:!1,title:"Memory Use",description:"File based factors",source:"@site/docs/guides/advanced/memory.md",sourceDirName:"guides/advanced",slug:"/guides/advanced/memory",permalink:"/octopus/docs/guides/advanced/memory",editUrl:"https://github.com/${organizationName}/${projectName}/edit/${branch}/website/docs/guides/advanced/memory.md",version:"current",frontMatter:{id:"memory",title:"Memory Use"},sidebar:"docs",previous:{title:"Multithreading",permalink:"/octopus/docs/guides/advanced/threading"},next:{title:"VCF Format",permalink:"/octopus/docs/guides/advanced/vcf"}},c=[],u={toc:c};function d(e){var t=e.components,r=(0,o.Z)(e,i);return(0,a.kt)("wrapper",(0,n.Z)({},u,r,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h4",{id:"file-based-factors"},"File based factors"),(0,a.kt)("p",null,"Octopus has been designed to minimise disk accesses by buffering frequently accessed resources in main memory. In other words, rather than making lots of short disk reads and using little main memory, octopus prefers to making few large disk reads, and use more main memory. This is beneficial for a number of reasons, especially in a cluster setting. There are essentially two resources which octopus actively buffers: read data and reference sequence. Both these can be controlled by the user:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"--max-reference-cache-footprint")," (",(0,a.kt)("inlineCode",{parentName:"li"},"-X"),") controls the reference buffer size."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"--target-read-buffer-footprint")," (",(0,a.kt)("inlineCode",{parentName:"li"},"-B"),") controls the read buffer size. This is not a hard limit, but for most normal samples octopus will respect this.")),(0,a.kt)("p",null,"Both options are specified in all the standard memory units (e.g. ",(0,a.kt)("inlineCode",{parentName:"p"},"500Mb"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"2G"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"4GB"),", etc). Note both buffer sizes are shared between threads, so ",(0,a.kt)("inlineCode",{parentName:"p"},"--target-read-buffer-footprint=2Gb")," using two threads would mean 1GB per thread. This is important is it determines the size of thread 'jobs', and can therefore have a significant impact on throughput and overall runtime."),(0,a.kt)("h4",{id:"other-factors"},"Other factors"),(0,a.kt)("p",null,"The other factors to consider when optimising memory usage are:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Multithreading: More threads means more memory overhead. A good starting point is to 'budget' an extra 100MB per additional thread, however, this will also depend on your data and the factors below, so some trial and error may be required."),(0,a.kt)("li",{parentName:"ul"},"Calling model: Simpler models use less memory."),(0,a.kt)("li",{parentName:"ul"},"Model setup: The parameterisation of the calling model can have a large influence on short term memory usage. For example setting ",(0,a.kt)("inlineCode",{parentName:"li"},"--max-joint-genotypes")," (in the trio or population model) to a very large memory may mean requests for large memory blocks.")))}d.isMDXComponent=!0}}]);