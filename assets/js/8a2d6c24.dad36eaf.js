(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[293],{3905:function(e,t,n){"use strict";n.d(t,{Zo:function(){return l},kt:function(){return m}});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var c=r.createContext({}),u=function(e){var t=r.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},l=function(e){var t=u(e.components);return r.createElement(c.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},p=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,c=e.parentName,l=s(e,["components","mdxType","originalType","parentName"]),p=u(n),m=a,h=p["".concat(c,".").concat(m)]||p[m]||d[m]||o;return n?r.createElement(h,i(i({ref:t},l),{},{components:n})):r.createElement(h,i({ref:t},l))}));function m(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,i=new Array(o);i[0]=p;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s.mdxType="string"==typeof e?e:a,i[1]=s;for(var u=2;u<o;u++)i[u]=n[u];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}p.displayName="MDXCreateElement"},2819:function(e,t,n){"use strict";n.r(t),n.d(t,{frontMatter:function(){return s},metadata:function(){return c},toc:function(){return u},default:function(){return d}});var r=n(2122),a=n(9756),o=(n(7294),n(3905)),i=["components"],s={id:"threading",title:"Multithreading"},c={unversionedId:"guides/advanced/threading",id:"guides/advanced/threading",isDocsHomePage:!1,title:"Multithreading",description:"Usage",source:"@site/docs/guides/advanced/threading.md",sourceDirName:"guides/advanced",slug:"/guides/advanced/threading",permalink:"/docs/guides/advanced/threading",editUrl:"https://github.com/${organizationName}/${projectName}/edit/${branch}/website/docs/guides/advanced/threading.md",version:"current",frontMatter:{id:"threading",title:"Multithreading"},sidebar:"docs",previous:{title:"Targeted Calling",permalink:"/docs/guides/advanced/targeted"},next:{title:"Memory Use",permalink:"/docs/guides/advanced/memory"}},u=[],l={toc:u};function d(e){var t=e.components,n=(0,a.Z)(e,i);return(0,o.kt)("wrapper",(0,r.Z)({},l,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h4",{id:"usage"},"Usage"),(0,o.kt)("p",null,"Octopus has built in multithreading capabilities, just add the ",(0,o.kt)("inlineCode",{parentName:"p"},"--threads")," command:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"$ octopus -R hs37d5.fa -I NA12878.bam --threads\n")),(0,o.kt)("p",null,"This will let octopus automatically decide how many threads to use (usually the number of available cores). However, a strict upper limit on the number of threads can also be used:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"$ octopus -R hs37d5.fa -I NA12878.bam --threads 4\n")),(0,o.kt)("h4",{id:"discussion"},"Discussion"),(0,o.kt)("p",null,"Using more than one threads is the simplest way to speed up variant calling. However, there is not always a linear payoff in runtime and the number of threads. Optimising thread throughput is challenging as it is highly data dependent. The best case scenario is to have each thread assigned single tasks of equal complexity so that they all finish simultaneously. Internally, octopus divides the calling region into chunks (genomic regions) with approximately the same number of reads. In particular each chunk has ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/luntergroup/octopus/wiki/Command-line-reference#option---target-read-buffer-memory"},(0,o.kt)("inlineCode",{parentName:"a"},"--target-read-buffer-memory"))," / ",(0,o.kt)("inlineCode",{parentName:"p"},"--threads")," worth of reads. Allowing octopus more read memory therefore increases the size of each chunk (assuming constant number of threads). Here are some consequences of larger chunks to consider:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Less thread management overhead (good)."),(0,o.kt)("li",{parentName:"ul"},"Less IO accesses (good)."),(0,o.kt)("li",{parentName:"ul"},"Higher chance of unbalanced workload (bad)."),(0,o.kt)("li",{parentName:"ul"},"More thread blocking due to IO constraint (bad).")),(0,o.kt)("p",null,"Some experimentation may be required to find the best thread/memory combination for your particular data."))}d.isMDXComponent=!0}}]);