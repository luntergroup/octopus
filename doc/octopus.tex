\documentclass{article}

\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage{graphicx} % Required to insert images
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in 

\linespread{1.1} % Line spacing

\DeclareMathOperator*{\argmax}{arg\,max}

\title{Octopus}
\author{Daniel Cooke}
\date{}

\begin{document}

\maketitle

This document describes in detail the algorithms and models implemented in the variant calling software Octopus.

\tableofcontents

\section{Introduction}

\section{Input}

\section{Read filtering and transformations}

\section{Candidate generation}

\subsection{Raw CIGAR alignement}

\subsection{Local re-assembley}

\subsection{Re-genotyping}

\subsection{Online}

\section{Haplotype generation}

\section{Haplotype filtering}

\section{Read error models}

\subsection{Re-mapping}

\subsection{HMM}

\section{Genotype models}

\subsection{Common definitions}

Although each genotype model is in a sense independent, they do share common attributes which we define here for brevity.

\begin{center}
\begin{tabular}{ll}
Constants & Description \\
\hline
$P_s$ & The ploidy of organism $s$\\
$h$ & A haplotype \\
$\mathbb{M_{e}}$ & The sequencing error model \\
\hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ll}
Latent variables & Description \\
\hline
$r$ & A sequencing read \\
\hline
\end{tabular}
\end{center}

Then the conditional probability of a read given a haplotype is:

\begin{equation}
\label{eq:read_prob}
    p(r | h, \mathbb{M_{e}})
\end{equation}

Which has already been described. For brevity, we will from here on ommit the $\mathbb{M_{e}}$ term and just write $p(r | h)$.

\subsection{Individual}

This model is designed to model reads coming from a single individual with a known fixed ploidy. It is the simplest genotype model that Octopus offers, and is also the fastest to compute. It is used by multiple Octopus variant callers.

\subsubsection{Definitions}

\begin{center}
\begin{tabular}{ll}
Constants & Description \\
\hline
$P$ & The individuals ploidy \\
$G$ & The number of genotypes \\
$\boldsymbol{\phi}$ & Vector of $G$ probabilities \\
$h_{ij}$ & The $j^{th}$ haplotype of the $i^{th}$ genotype \\
$\mathcal{R}$ & The set of sequencing reads for the individual \\
$N$ & $\mathcal{R}$ \\
\hline
\end{tabular}
\end{center}

Note that $h_{ij}$ is technically a deterministic surjective function $h_{ij} = f(\boldsymbol{g}, i, j)$ which maps genotypes to haplotypes, but for brevity we just write $h_{ij}$.

\begin{center}
\begin{tabular}{ll}
Observed latent variables & Description \\
\hline
$\mathcal{R}$ & The set of sequencing reads for the individual \\
$N$ & $\mathcal{R}$ \\
\hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ll}
Hidden latent variables & Description \\
\hline
$\boldsymbol{g}$ & Binary (1-of-$G$) \\
\hline
\end{tabular}
\end{center}

\subsubsection{Marginal distrubtions}

The marginal distribtion for $\boldsymbol{g}$ is categorical:

\begin{equation}
\label{eq:ind_g_marginal}
    p(\boldsymbol{g} | \boldsymbol{\phi}) = \prod_{i = 1}^G \phi_i^{g_i}
\end{equation}

The marginal distrubtion for a single read $r$ is a mixture distribtuion, which follows from the assumption that the haplotypes that make up a known ploidy genotype are exchangable, and thus we must assign them equal probabilities of being sequenced:

\begin{equation}
\label{eq:ind_read_marginal}
 p(r | g_i = 1) = \frac{1}{P} \sum^P_{k = 1} p(r | h_{ik})
\end{equation}

which can also be written as:

\begin{equation}
\label{eq:ind_read_marginal2}
 p(r | \boldsymbol{g}) = \prod_{i = 1}^G \frac{1}{P} \sum^P_{k = 1} p(r | h_{ik})^{g_i}
\end{equation}

\subsubsection{Joint distribution}

\begin{equation}
\label{eq:ind_jp}
 p(\mathcal{R}, \boldsymbol{g}) = \prod_{i = 1}^G \phi_i^{g_i} \prod^N_{n = 1} \frac{1}{P} \sum^P_{k = 1} p(r_n | h_{ik})^{g_i}
\end{equation}

\subsubsection{Posterior distribution}

\begin{equation}
\label{eq:ind_post}
 p(\boldsymbol{g} | \mathcal{R}) = \frac{\prod_{i = 1}^G \phi_i \prod^N_{n = 1} \frac{1}{P} \sum^P_{k = 1} p(r_n | h_{ik})^{g_i}}{\sum_{\boldsymbol{g}'}\prod_{i = 1}^G \phi_i \prod^N_{n = 1} \frac{1}{P} \sum^P_{k = 1} p(r_n | h_{ik})^{g_i}}
\end{equation}

We can factor out the constant $P$ term, and write more compactly as:

\begin{equation}
\label{eq:ind_post2}
 p(g_i = 1 | \mathcal{R}) = \frac{\phi_i \prod^N_{n = 1} \sum^P_{k = 1} p(r_n | h_{ik})}{\sum_{j = 1}^G \phi_i \prod^N_{n = 1} \sum^P_{k = 1} p(r_n | h_{ik})}
\end{equation}

\subsubsection{Evidence}

The evidence for the model is simply the denominator of (\ref{eq:ind_post}):

\begin{equation}
\label{eq:ind_ev}
 p(\mathcal{R}) = \sum_{i = 1}^G \phi_i \prod^N_{n = 1} \frac{1}{P} \sum^P_{k = 1} p(r_n | h_{ik})
\end{equation}

\subsection{Population}

\subsection{CNV}

The copy-number-variant genotype model attempts to model copy number changes in a single individual from which multiple samples have been taken. This model can be contrasted with the individual model which assumes the haplotypes in the individuals germline genotype are present in a $1:1$ ratio (for diploid cases).

\subsubsection{Definitions}

\begin{center}
\begin{tabular}{ll}
Constants & Description \\
\hline
$P$ & The individuals germline ploidy \\
$S$ & The number of samples for the individual \\
$G$ & The number of genotypes \\
$\boldsymbol{\phi}$ & Vector of $G$ probabilities \\
$h_{ij}$ & The $j^{th}$ haplotype of the $i^{th}$ genotype \\
$\boldsymbol{\alpha_s}$ & A vector of $P$ Dirichlet counts \\
\hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ll}
Observed latent variables & Description \\
\hline
$\mathcal{R}_s$ & The set of sequencing reads for sample $s$ of the individual \\
$N_s$ & $\mathcal{R}_s$ \\
\hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ll}
Hidden latent variables & Description \\
\hline
$\boldsymbol{g}$ & Binary (1-of-$G$) \\
$\boldsymbol{\pi}_s$ & A vector of $P$ probabilities of sequencing each haplotype in the individuals genotype \\
$\boldsymbol{z}_{sn}$ & Binary (1-of-$P$), specifying which haplotype was sequenced in read $n$ of sample $s$ \\
\hline
\end{tabular}
\end{center}

\subsubsection{Marginal distrubtions}

The marginal distribtion for $\boldsymbol{g}$ is categorical:

\begin{equation}
\label{eq:cnv_g_marginal}
    p(\boldsymbol{g} | \boldsymbol{\phi}) = \prod_{i = 1}^G \phi_i^{g_i}
\end{equation}

The marginal distribution of $\phi$ is assumed to be Dirichlet, this is mostly because it simplifies the mathematics.

\begin{equation}
\label{eq:cnv_pi_marginal}
    p(\boldsymbol{\pi} | \boldsymbol{\alpha}) = \text{Dir}(\boldsymbol{\pi} | \boldsymbol{\alpha}) = \frac{1}{\text{B}(\boldsymbol{\alpha})} \prod_{k = 1}^P \pi_k^{\alpha_k - 1}
\end{equation}

where $\text{B}(\boldsymbol{\alpha})$ is the multivariate Beta function:

\begin{equation}
\label{eq:beta_func}
    \text{B}(\boldsymbol{\alpha}) = \frac{\prod_{k = 1}^K \Gamma(\alpha_k)}{\Gamma(\sum_{k = 1}^K \alpha_k)}
\end{equation}

The marginal distribution for each $\boldsymbol{z}$ is categorical:

\begin{equation}
\label{eq:cnv_z_marginal}
    p(\boldsymbol{z} | \boldsymbol{\pi}) = \prod_{k = 1}^P \pi_k^{z_k}
\end{equation}

The marginal distrubtion for a single read $r$ is a mixture distribtuion:

\begin{align}
\label{eq:cnv_read_marginal}
 p(r | \boldsymbol{\pi}, g_i = 1) &= \sum_{\boldsymbol{z}} \sum^P_{k = 1} p(\boldsymbol{z} | \boldsymbol{\pi}) p(r | \boldsymbol{z}, h_{ik}) \\
 &= \sum^P_{k = 1} \pi_k p(r | h_{ik})
\end{align}

\subsubsection{Joint distribution}

\begin{align}
\label{eq:cnv_jp}
 p(\mathcal{R}, \boldsymbol{g}, \boldsymbol{Z}, \boldsymbol{\pi} | \boldsymbol{\alpha}, \boldsymbol{\phi}) &= p(\boldsymbol{g} | \boldsymbol{\phi}) \prod_{s = 1}^S p(\boldsymbol{\pi}_s | \boldsymbol{\alpha}_s) p(\boldsymbol{Z}_s | \boldsymbol{\pi}_s) p(\mathcal{R}_s | \boldsymbol{Z}_s, \boldsymbol{g}) \\
 &= \prod^G_{i = 1} \phi_i^{g_i} \prod_{s = 1}^S \frac{1}{\text{B}(\boldsymbol{\alpha}_s)} \prod_{k = 1}^P \pi_{sk}^{\alpha_{sk} - 1} \prod_{n = 1}^{N_s} \prod_{k = 1}^P \pi_{sk}^{z_{snk}} \prod^G_{i = 1} \prod_{n = 1}^{N_s} \prod_{k = 1}^P p(r_{sn} | h_{ik})^{g_i z_{snk}}
\end{align}

\subsubsection{Posterior distribution}

We are interested in the posterior distrubtions of $\boldsymbol{g}$ and $\boldsymbol{\pi}$, which due to the hidden latent variables $\boldsymbol{z}$ cannot be evaluated in closed form, we therefore resort to approximations. We choose a Variational Bayes approximation because it is determinstic and faster to compute than Monte Carlo based methods. A description of Variational Bayes is given in the appendix.

To evaluate approximate posteior distribtions in the Variational Bayes framework we need to factorise (\ref{eq:cnv_jp}) into independent factors, there is only one possability here:

\begin{equation}
\label{eq:cnv_vb_factors}
    q(\boldsymbol{g}, \boldsymbol{Z}, \boldsymbol{\pi}) = q(\boldsymbol{g}) \prod_{s = 1}^S q(\boldsymbol{Z}_s) q(\boldsymbol{\pi}_s)
\end{equation}

where non-latent variables have been ommited for brevity. We evaluate the optimal factors in tern.

\begin{align}
\label{eq:cnv_ln_q_z}
\ln q^*(\boldsymbol{Z}_s) &= \mathbb{E}_{\boldsymbol{g}, \boldsymbol{\pi}, \boldsymbol{Z}_{s' \ne s}} [\ln p(\mathcal{R}, \boldsymbol{g}, \boldsymbol{Z}, \boldsymbol{\pi} | \boldsymbol{\alpha}, \boldsymbol{\phi})] + \text{const} \\
&= \mathbb{E}[\ln p(\boldsymbol{Z}_s | \boldsymbol{\pi}_s)] + \mathbb{E}[\ln p(\mathcal{R}_s | \boldsymbol{Z}_s, \boldsymbol{g})] + \text{const} \\
&= \sum_{n = 1}^{N_s} \sum_{k = 1}^P z_{snk} \mathbb{E}[\ln \pi_{sk}] + \sum_{n = 1}^{N_s} \sum_{k = 1}^P \sum_{i = 1}^G \mathbb{E}[g_i] \ln p(r_{sn} | h_{ik})^{g_i z_{snk}} + \text{const} \\
&= \sum_{n = 1}^{N_s} \sum_{k = 1}^P z_{snk} \ln \rho_{snk} + \text{const}
\end{align}

where we have defined

\begin{equation}
\label{eq:cnv_ln_rho}
\ln \rho_{snk} = \ln \tilde{\pi}_{sk} + \sum_{i = 1}^G \phi_i \ln p(r_{sn} | h_{ik})
\end{equation}

and

\begin{equation}
\label{eq:cnv_ex_ln_pi}
\ln \tilde{\pi}_{sk} = \psi(\alpha_{sk}) - \psi(\hat{\alpha}_s)
\end{equation}

where $\psi$ is the digamma function and $\hat{\alpha}_{s} = \sum_{k = 1}^P \alpha_{sk}$. Exponentiating both sides of (\ref{eq:cnv_ln_q_z}) and normalisng we obtain:

\begin{equation}
\label{eq:cnv_q_z}
q^*(\boldsymbol{Z}_s) = \prod_{n = 1}^{N_s} \prod_{k = 1}^P \tau_{snk}^{z_{snk}}
\end{equation}

where

\begin{equation}
\label{eq:cnv_tau}
\tau_{snk} = \frac{\rho_{snk}}{\sum_{j = 1}^P \rho_{snj}}
\end{equation}

Next we look at each $q^*(\boldsymbol{\pi}_s)$:

\begin{align}
\label{eq:cnv_ln_q_pi}
\ln q^*(\boldsymbol{\pi}_s) &= \mathbb{E}_{\boldsymbol{g}, \boldsymbol{\pi}_s' \ne s}, \boldsymbol{Z} [\ln p(\mathcal{R}, \boldsymbol{g}, \boldsymbol{Z}, \boldsymbol{\pi} | \boldsymbol{\alpha}, \boldsymbol{\phi})] + \text{const} \\
&= \mathbb{E}[\ln p(\boldsymbol{\pi}_s | \boldsymbol{\alpha}_s)] + \mathbb{E}[\ln p(\boldsymbol{Z}_s | \boldsymbol{\pi}_s)] + \text{const} \\
&= \sum_{k = 1}^P (\alpha_k - 1) \ln \pi_{sk} + \sum_{k = 1}^P \sum_{n = 1}^{N_s} \tau_{snk} \ln \pi_{sk} + \text{const}
\end{align}

and therefore

\begin{align}
\label{eq:cnv_q_pi}
q^*(\boldsymbol{\pi}_s) &= \prod_{k = 1}^P \pi_{sk}^{\alpha_{sk} - 1} \prod_{k = 1}^P \pi_{sk}^{\sum_{n = 1}^{N_s} \tau_{snk}} \\
&=  \prod_{k = 1}^P \pi_{sk}^{\alpha_{sk} + \sum_{n = 1}^{N_s} \tau_{snk} - 1}
\end{align}

Which we can see from inspection is another Dirichlet distrubtion with pseudo-counts:

\begin{equation}
\label{eq:cnv_new_counts}
\alpha_{sk}^{post} = \alpha_{sk}^{prior} + \hat{N}_{sk}
\end{equation}

where $\hat{N}_{sk} = \sum_{n = 1}^{N_s} \tau_{snk}$. Finally we evaluate $q^*(\boldsymbol{g})$:

\begin{align}
\label{eq:cnv_ln_q_g}
\ln q^*(\boldsymbol{g}) &= \mathbb{E}_{\boldsymbol{\pi}, \boldsymbol{Z}} [\ln p(\mathcal{R}, \boldsymbol{g}, \boldsymbol{Z}, \boldsymbol{\pi} | \boldsymbol{\alpha}, \boldsymbol{\phi})] + \text{const} \\
&= \mathbb{E}[\ln p(\boldsymbol{g} | \boldsymbol{\phi})] + \mathbb{E}[\ln p(\mathcal{R}_s | \boldsymbol{Z}_s, \boldsymbol{g})] + \text{const} \\
&= \mathbb{E} \left [\sum_{i = 1}^G g_i \ln \phi_i \right] + \mathbb{E} \left[ \sum_{n = 1}^{N_s} \sum_{k = 1}^P \sum_{i = 1}^G g_i z_{snk} \ln p(r_{sn} | h_{ik}) \right] + \text{const} \\
&= \sum_{i = 1}^G \mathbb{E}[g_i] \ln \phi_i +  \sum_{i = 1}^G \mathbb{E}[g_i] \sum_{n = 1}^{N_s} \sum_{k = 1}^P \mathbb{E}[z_{snk}] \ln p(r_{sn} | h_{ik}) + \text{const} \\
&= \sum_{i = 1}^G \mathbb{E}[g_i] \left\{ \ln \phi_i + \sum_{n = 1}^{N_s} \sum_{k = 1}^P \tau_{snk} \ln p(r_{sn} | h_{ik}) \right\} + \text{const}
\end{align}

So we immediatly see that

\begin{equation}
\label{eq:cnv_q_g}
q^*(\boldsymbol{g}) \propto \prod_{i = 1}^G \theta^{g_i}
\end{equation}

where

\begin{equation}
\label{eq:cnv_theta}
\ln \theta_i = \ln \phi_i + \sum_{n = 1}^{N_s} \sum_{k = 1}^P \tau_{snk} \ln p(r_{sn} | h_{ik})
\end{equation}

\subsubsection{Evidence}

The Variational Bayes framework also gived a lower-bound on the evidence for the full posterior distribtion. The lower bound is given by:

\begin{align}
\label{eq:cnv_evidence}
\begin{split}
\mathcal{L} &= \mathbb{E}[\ln p(\mathcal{R}, \boldsymbol{g}, \boldsymbol{Z}, \boldsymbol{\pi} | \boldsymbol{\alpha}, \boldsymbol{\phi})] - \mathbb{E} [\ln q^*(\boldsymbol{g}, \boldsymbol{Z}, \boldsymbol{\pi})] \\
&= \mathbb{E}[\ln p(\boldsymbol{g} | \boldsymbol{\phi})] + \sum_{s = 1}^S \left\{ \mathbb{E}[\ln p(\boldsymbol{\pi}_s | \boldsymbol{\alpha}_s)] + \mathbb{E}[\ln p(\boldsymbol{Z}_s | \boldsymbol{\pi}_s)] + \mathbb{E}[\ln p(\mathcal{R}_s | \boldsymbol{Z}_s, \boldsymbol{g})] \right\} \notag \\
     &\phantom{{}=\mathbb{E}[\ln p(\boldsymbol{g} | \boldsymbol{\phi})] } - \mathbb{E}[\ln q^*(\boldsymbol{g})] - \sum_{s = 1}^S \left\{ \mathbb{E}[\ln q^*(\boldsymbol{\pi}_s) + \mathbb{E}[\ln q^*(\boldsymbol{Z}_s)] \right\}
\end{split}
\end{align}

Where each expectation is taken with respect to $q^*$. Evaluating each term seperatly

\begin{equation}
\mathbb{E}[\ln p(\boldsymbol{g} | \boldsymbol{\phi})] = \mathbb{E}\left[\sum_{i = 1}^P g_i \ln \phi_i\right] = \sum_{i = 1}^P \mathbb{E}[g_i] \ln \phi_i = \sum_{i = 1}^P \phi_i^{post} \ln \phi_i^{prior}
\end{equation}

\begin{align}
\mathbb{E}[\ln p(\boldsymbol{\pi}_s | \boldsymbol{\alpha}_s)] &= \mathbb{E}\left[\sum_{k = 1}^P (\alpha_k^{prior} - 1) \ln \pi_{sk} - \ln \text{B}(\boldsymbol{\alpha}^{prior})\right] \notag \\ 
&= \sum_{k = 1}^P (\alpha_k^{prior} - 1) \mathbb{E} [\ln \pi_{sk}] - \ln \text{B}(\boldsymbol{\alpha}^{prior}) \notag \\
 &= \sum_{k = 1}^P (\alpha_k^{prior} - 1) \ln \tilde{\pi}_{sk} - \ln \text{B}(\boldsymbol{\alpha}^{prior})
\end{align}

\begin{equation}
\mathbb{E}[\ln p(\boldsymbol{Z}_s | \boldsymbol{\pi}_s)] = \mathbb{E}\left[\sum_{n = 1}^{N_s} \sum_{k = 1}^P z_{snk} \ln \pi_{sk} \right] 
= \sum_{n = 1}^{N_s} \sum_{k = 1}^P \mathbb{E}[z_{snk} \ln \pi_{sk}]
= \sum_{n = 1}^{N_s} \sum_{k = 1}^P \tau_{snk} \ln \tilde{\pi}_{sk}
\end{equation}

as $\tau$ and $z$ are independent under $q$.

\begin{equation}
\mathbb{E}[\ln p(\mathcal{R}_s | \boldsymbol{Z}_s, \boldsymbol{g})] = \mathbb{E}\left[ \sum_{i = 1}^G \sum_{n = 1}^{N_s} \sum_{k = 1}^P g_i z_{snk} \ln p(r_{sn} | h_{ik}) \right] = \sum_{i = 1}^G \phi_i^{post} \sum_{n = 1}^{N_s} \sum_{k = 1}^P \tau_{snk} \ln p(r_{sn} | h_{ik})
\end{equation}

\begin{equation}
\mathbb{E}[\ln q^*(\boldsymbol{g})] = \mathbb{E}\left[\sum_{i = 1}^P g_i \ln \phi_i\right] = \sum_{i = 1}^P \mathbb{E}[g_i] \ln \phi_i = \sum_{i = 1}^P \phi_i^{post} \ln \phi_i^{post}
\end{equation}

\begin{align}
\mathbb{E}[\ln q^*(\boldsymbol{\pi}_s)] =& \mathbb{E}\left[\sum_{k = 1}^P (\alpha_k^{post} - 1) \ln \pi_{sk} - \ln \text{B}(\boldsymbol{\alpha}^{post})\right] \notag \\ 
&= \sum_{k = 1}^P (\alpha_k^{post} - 1) \mathbb{E} [\ln \pi_{sk}] - \ln \text{B}(\boldsymbol{\alpha}^{post}) \notag \\
 &= \sum_{k = 1}^P (\alpha_k^{post} - 1) \ln \tilde{\pi}_{sk} - \ln \text{B}(\boldsymbol{\alpha}^{post})
\end{align}

\begin{equation}
\mathbb{E}[\ln q^*(\boldsymbol{Z}_s)] = \mathbb{E}\left[\sum_{n = 1}^{N_s} \sum_{k = 1}^P z_{snk} \ln \tau_{snk} \right] = \sum_{n = 1}^{N_s} \sum_{k = 1}^P \tau_{snk} \ln \tau_{snk}
\end{equation}

These equations can also be used as a test of correctness of the implementation, as $\mathcal{L}$ should increase on each iteration.

\subsection{Somatic}

The somatic genotype model is identical to the CNV model, with a slight modification to the sample space. Specifically the genotype variable $\boldsymbol{g}$ from the CNV model is replaced with a 'somatic' genotype $\tilde{\boldsymbol{g}}$ which is simply a $1$-of-$P + 1$ binary random variable. A further restriction of the sample space $\tilde{\boldsymbol{g}}$ is that $h_{i(P + 1)} \ne h_{ij} \forall j \le P$, which simply states that the last (somatic) component always contains a novel haplotype (i.e. not present in the germline).

\section{Variant callers}

\subsection{Individual}

\subsection{Population}

\subsection{Cancer}

The cancer caller is designed to detect somatic mutations in a set of tumour samples from a single indivudal. The set of samples may also include a single normal sample, which is assumed to be tumour free.

\subsubsection{Pre-processing}

A set of candidate germline and 'cancer' genotypes is generated from the set of candidate haplotypes. If the number of haplotypes is strictly greater than the organism ploidy then each germline genotype will be present in the set of cancer genotypes at-least once. 

Each set of genotypes may then be filtered to reduce the complexity of the model fitting step.

\subsubsection{Model comparison}

We then fit the individual $\mathcal{M}_g$, CNV $\mathcal{M}_c$, and somatic $\mathcal{M}_s$ models to the data. For the individual case all reads are pooled together, which as can be seen from \ref{eq:ind_jp} results in the same likelihood function as treating each sample seperatly. We then calculate the posterior probability of each model by evaluating Bayes theorem:

\begin{equation}
\label{eq:bayes_model}
p(\mathcal{M} | \mathcal{D}) = \frac{p(\mathcal{M})p(\mathcal{D} | \mathcal{M})}{p(\mathcal{D})}
\end{equation}



\section{Phasing}

\section{Variant call filtering}

\section{Output}

\section{Appendix}

\subsection{Variational Bayes}

Given a probability model $p(\boldsymbol{X}, \boldsymbol{Z})$ where $\boldsymbol{X}$ is observed and $\boldsymbol{Z}$ are latent, the true posterior density $p(\boldsymbol{Z} | \boldsymbol{X})$ can be approximated with another distribution $q(\boldsymbol{Z})$ subject to some measure of similarity. A natural choice of similarity is the Kullbackâ€“Leibler divergence

\begin{equation}
\label{eq:kl}
   \text{KL} (q\; ||\; p) = -\int q(\boldsymbol{Z}) \ln \frac{p(\boldsymbol{Z} | \boldsymbol{X})}{q(\boldsymbol{Z})} d\boldsymbol{Z}
\end{equation}

This measures the additional amount of information (in nats) required to generate codes from $q$ rather than $p$, it satisfies $\text{KL}(q\; ||\; p) \ge 0$, with equality when $p = q$. So we actually try to minimise this quantity.

We now partition the latent variables $\boldsymbol{Z}$ into a set of disjoint groups denoted by $\boldsymbol{Z_i}$ where $i = 1, \dots, M$ and assume that the $q$ distribution factorises into a product of these groups, i.e.

\begin{equation}
\label{eq:q}
  q(\boldsymbol{Z}) = \prod_{i = 1}^M q_i(\boldsymbol{Z_i})
\end{equation}

This is the only assumption made, in particular the functional form of each $q_i$ is not constrained. The idea is then to optimise each group in tern, which can formally be solved using calculus of variations, but we can to see that by substituting (\ref{eq:q}) into (\ref{eq:kl}) and separating one group $\boldsymbol{Z_j}$ that

\begin{align}
    \text{KL}(q\; ||\; p) &= -\int \prod_{i = 1}^M q_i \left\{ \ln p(\boldsymbol{Z} | \boldsymbol{X}) - \sum_i \ln q_i) \right\} d \boldsymbol{Z}\notag \\
    &= -\int q_j \left\{ \int \ln p(\boldsymbol{Z} | \boldsymbol{X}) \prod_{i \ne j} d \boldsymbol{Z_i} \right\} d \boldsymbol{Z_j} + \int q_j \ln q_j d \boldsymbol{Z_j} + \text{const}\notag\\
    &= -\int q_j \mathbb{E}_{i \ne j}[\ln p(\boldsymbol{Z} | \boldsymbol{X})] d \boldsymbol{Z_j} + \int q_j \ln q_j d \boldsymbol{Z_j} + \text{const}\notag\\
    &= \text{KL}(q_j\; ||\; \exp (\mathbb{E}_{i \ne j}[\ln p(\boldsymbol{Z} | \boldsymbol{X})])) + \text{const}
\end{align}

Clearly the $q_j$ which minimises this quantity is when $q_j = \exp(\mathbb{E}_{i \ne j}[\ln p(\boldsymbol{Z} | \boldsymbol{X})])$ and therefore we find the optimal $q_j$

\begin{equation}
q^*_j(\boldsymbol{Z_j}) = \mathbb{E}_{i \ne j}[\ln p(\boldsymbol{Z} | \boldsymbol{X})] + \text{const}
\end{equation}

or equvilantly if $p(\boldsymbol{X})$ is absorbed into the constant then

\begin{equation}
\label{eq:q_opt}
q^*_j(\boldsymbol{Z_j}) = \mathbb{E}_{i \ne j}[\ln p(\boldsymbol{X}, \boldsymbol{Z})] + \text{const}
\end{equation}

Note these equations do not represent an explicit solution because they are interdependent. The variational Bayes algorithm therefore proceeds similar to EM by cycling through each group, updating $q^*$, and repeating until convergence. It can be shown that $\text{KL}(q\; ||\; p)$ decreases at each step.

\end{document}
